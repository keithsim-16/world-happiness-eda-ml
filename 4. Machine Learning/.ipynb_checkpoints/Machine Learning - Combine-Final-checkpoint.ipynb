{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ec3765",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "### Datasets Used:\n",
    "> 2015 UNData: (2015 is the only year where all variables are reported, since the World Happiness Report started in 2012) \n",
    "> - Population, surface area and density  \n",
    "> - Population in the capital city, urban and rural areas    \n",
    "> - GDP and GDP per capita    \n",
    "> - GVA by kind of economic activity  \n",
    "> - Education at the primary, secondary and tertiary levels  \n",
    "> - Employment by economic activity  \n",
    "> - Water supply and sanitation coverage  \n",
    "> - Internet usage  \n",
    "> - Population growth, fertility, life expectancy and mortality\n",
    ">\n",
    "> Source: https://data.un.org/\n",
    "\n",
    "> 2017 UNData:\n",
    "> - Country Statistics - UNData  \n",
    ">\n",
    "> Source: https://www.kaggle.com/datasets/sudalairajkumar/undata-country-profiles\n",
    "\n",
    "> World Happiness Report 2015 & 2017:  \n",
    "Source: https://www.kaggle.com/datasets/unsdsn/world-happiness  \n",
    "\n",
    "### Essential Libraries\n",
    "\n",
    "Importing the essential Python Libraries.\n",
    "\n",
    "> NumPy : Library for Numeric Computations in Python  \n",
    "> Pandas : Library for Data Acquisition and Preparation  \n",
    "> Matplotlib : Low-level library for Data Visualization  \n",
    "> Seaborn : Higher-level library for Data Visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "445363af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4928b7f",
   "metadata": {},
   "source": [
    "### Import Dataset\n",
    "> Import dataset created after data processing & cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d92985a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19440/4058898467.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dataset.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset.csv'"
     ]
    }
   ],
   "source": [
    "Data = pd.read_csv('Dataset.csv')\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e55d5b",
   "metadata": {},
   "source": [
    "In this machine learning section, we will predict the happiness score of a country which is in the form of numerical value. Hence, our model selection will be based on this idea. \n",
    "\n",
    "We will be exploring `3` machine learning models.\n",
    "<br>\n",
    "\n",
    "For the supervised learning, we choose `ElasticNet Regression` and `Random Forest Regressorion`.\n",
    "\n",
    "For the unsupervised learning, we choose `KMeans Clustering (Centroid Based)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd64fb",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.  ElasticNet Regression\n",
    "\n",
    ">- ElasticNet is a combination of Least Squares and the regression penalty of both Lasso and Ridge Regression.\n",
    ">- It allows us to combine the strengths of lasso and ridge regression into one.\n",
    ">- Cross validation is used to tune the hyperparameters to make more accurate predictions.\n",
    ">- Least Squares is the basic linear regression model.\n",
    ">- Lasso (L1 Regularization) and Ridge Regression (L2 Regularization) are very similar as they both introduces a small amount of bias in an attempt to reduce the variance.\n",
    ">- The only difference is that Lasso Regression can shrink less important features coefficient all the way to zero, which helps with feature selection.\n",
    "\n",
    ">Why we choose this model? \n",
    ">- We have a small sample size of ~300.\n",
    ">- Only a few features that are highly correlated were chosen as predictors.\n",
    "\n",
    "\n",
    ">We will be using top 6 variables that are highly correlated with happiness \n",
    "score as the independent variables.\n",
    "\n",
    ">Response Variable: `happiness.score`  \n",
    ">Predictor Features: `Employment: Services (% of employed)`, `Life expectancy at birth (females, years)`,    \n",
    "`Life expectancy at birth (males, years)`, `Education: Secondary gross enrol. ratio (male per 100 pop.)`,     \n",
    "`Education: Secondary gross enrol. ratio (female per 100 pop.)`,     \n",
    "`Individuals using the Internet (per 100 inhabitants)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f786d52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = pd.DataFrame(Data[\"happiness.score\"])\n",
    "X = pd.DataFrame(Data[[\"Employment: Services (% of employed)\", \n",
    "                       \"Life expectancy at birth (females, years)\", \n",
    "                       \"Life expectancy at birth (males, years)\", \n",
    "                       \"Education: Secondary gross enrol. ratio (male per 100 pop.)\", \n",
    "                       \"Education: Secondary gross enrol. ratio (female per 100 pop.)\", \n",
    "                       \"Individuals using the Internet (per 100 inhabitants)\"]])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0a3e0",
   "metadata": {},
   "source": [
    "Split the dataset into train and test sets at a 80:20 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the Dataset into random Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# Check the sample sizes\n",
    "print(\"Train Set :\", X_train.shape, y_train.shape)\n",
    "print(\"Test Set  :\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2becf644",
   "metadata": {},
   "source": [
    "### Basic Exploration\n",
    "Basic statistical exploration and visualization on the Train Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed48765",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c8f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ce7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the distribution of Response\n",
    "f, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "sb.boxplot(data = y_train, orient = \"h\", ax = axes[0])\n",
    "sb.histplot(data = y_train, ax = axes[1])\n",
    "sb.violinplot(data = y_train, orient = \"h\", ax = axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12e72e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Draw the distributions of all Predictors\n",
    "f, axes = plt.subplots(6, 3, figsize=(18, 25))\n",
    "\n",
    "count = 0\n",
    "for var in X_train:\n",
    "    sb.boxplot(data = X_train[var], orient = \"h\", ax = axes[count,0])\n",
    "    sb.histplot(data = X_train[var], ax = axes[count,1])\n",
    "    sb.violinplot(data = X_train[var], orient = \"h\", ax = axes[count,2])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96106414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Response and the Predictors\n",
    "train_df = pd.concat([y_train, X_train], axis = 1).reindex(y_train.index)\n",
    "\n",
    "# Relationship between Response and the Predictors\n",
    "sb.pairplot(data = train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9c164",
   "metadata": {},
   "source": [
    "### ElasticNet Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b466be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet,ElasticNetCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from numpy import arange\n",
    "# Split the Dataset into random Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=0)\n",
    "\n",
    "\n",
    "#elastic = ElasticNet(alpha=0.01)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=0)\n",
    "ratios = arange(0, 1, 0.01)\n",
    "alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "#### ElasticNet Regression\n",
    "elastic = ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=-1,random_state=0)\n",
    "\n",
    "elastic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e2e38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Intercept: \", elastic.intercept_)\n",
    "print(\"Coefficients: \")\n",
    "pd.DataFrame(list(zip(X_train.columns, elastic.coef_)), columns = [\"Predictors\", \"Coefficients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c43d2",
   "metadata": {},
   "source": [
    "> The two columns have coefficients of 0 because the lasso regression of elastic net is capable of shrinking less important features coefficient all the way to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e336b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Total values from Predictors\n",
    "y_train_pred = elastic.predict(X_train)\n",
    "y_test_pred = elastic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,4))\n",
    "x_ax = range(len(X_test))\n",
    "plt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, y_test_pred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.xlabel('id')\n",
    "plt.ylabel('happiness score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e96d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the measured and predicted result in a graph\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_train, y_train_pred, edgecolors=(0, 0, 0),color = 'blue')\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, y_test_pred, edgecolors=(0, 0, 0), color = 'red')\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", elastic.score(X_train, y_train))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mean_squared_error (y_train, y_train_pred)))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", elastic.score(X_test, y_test))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mean_squared_error (y_test, y_test_pred)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d153a5",
   "metadata": {},
   "source": [
    ">- The RMSE value of the test dataset is still very high which is 0.683\n",
    ">- The R^2 which represent the accuracy of this model is only 59%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56987597",
   "metadata": {},
   "source": [
    "> <b>Pros of Elastic Net Regression :</b>\n",
    ">- Combine the strengths of lasso and ridge regression into one.\n",
    "\n",
    "><b>Cons of Elastic Net Regression :</b>\n",
    ">-  Low Accuracy and High RMSE value\n",
    ">-  Computationally more expensive than Lasso or Ridge regression.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28210f22",
   "metadata": {},
   "source": [
    "## 2. Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72749d5",
   "metadata": {},
   "source": [
    "The Random Forest Regressor is a supervised learning algorithm that uses ensemble learning method for regression. \n",
    "\n",
    "<b> Ensemble learning method</b> is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model.\n",
    "\n",
    "This model is a bootstrap (bagging) technique. Bootstrap refers to random sampling with replacement of a small subset of data from the data set, which allows us to better understand the bias and the variance within the data set.\n",
    "\n",
    "<b>Steps: </b>\n",
    "1. Construct a large number of decision trees at training time.\n",
    "2. Each tree is created from a different sample of data and at each node, a different sample of features is selected for splitting. \n",
    "3. Each of the trees makes its own individual prediction. \n",
    "4. These predictions are then averaged to produce a single result. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6c561",
   "metadata": {},
   "source": [
    "### A. Model with All Independent Variables\n",
    "#### Larger tree (max_depth = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c70948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Response and Predictors\n",
    "df = df.drop('country', axis = 1)\n",
    "df_copy = pd.get_dummies(df)\n",
    "\n",
    "y = df['happiness.score']\n",
    "X = df_copy.drop('happiness.score', axis=1)\n",
    "\n",
    "X_list = list(X.columns)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003322d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestRegressor model from Scikit-Learn\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Fitting the data\n",
    "regressor = RandomForestRegressor (n_estimators = 100, # n_estimators denote number of trees\n",
    "                                   random_state = 0, # random_state parameter initialize the internal random number generator \n",
    "                                  max_depth = 20\n",
    "                                  )\n",
    "# Fit Random Forest on Train Data\n",
    "regressor.fit(X_train,y_train)\n",
    "                                   \n",
    "# Predict the Response corresponding to Predictors\n",
    "y_train_pred = regressor.predict (X_train)\n",
    "y_test_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fad17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the trained Decision Tree\n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "\n",
    "f = plt.figure(figsize=(500,400))\n",
    "# Pull out one tree from the forest\n",
    "tree = regressor.estimators_[5]\n",
    "\n",
    "# Export the image to a dot file\n",
    "tree=export_graphviz(tree,\n",
    "                     feature_names = X_list, \n",
    "                     rounded = True, \n",
    "                     filled = True)\n",
    "\n",
    "\n",
    "import graphviz \n",
    "graphviz.Source(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2376e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,4))\n",
    "prediction, = plt.plot(y_test_pred, label = \"prediction\")\n",
    "original, = plt.plot(y_test, label = 'original')\n",
    "first_legend= plt.legend(handles=[prediction])\n",
    "ax = plt.gca().add_artist(first_legend)\n",
    "plt.xlabel('id')\n",
    "plt.ylabel('happiness score')\n",
    "plt.legend(handles=[original],loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68730fd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19440/4075987148.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error checking\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Regression Accuracy (R^2) \\t:\", regressor.score(X_train, y_train))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error (y_train, y_train_pred))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mean_squared_error (y_train, y_train_pred)))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Regression Accuracy (R^2) \\t:\", regressor.score(X_test, y_test))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error (y_test, y_test_pred))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mean_squared_error (y_test, y_test_pred)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922724c",
   "metadata": {},
   "source": [
    "#### Smaller tree (max_depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a69ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['happiness.score']\n",
    "X = df_copy.drop('happiness.score', axis=1)\n",
    "\n",
    "X_list = list(X.columns)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "# Import RandomForestRegressor model from Scikit-Learn\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Fitting the data\n",
    "regressor = RandomForestRegressor (n_estimators = 100, # n_estimators denote number of trees\n",
    "                                   random_state = 0, # random_state parameter initialize the internal random number generator \n",
    "                                  max_depth = 3)\n",
    "# Fit Random Forest on Train Data\n",
    "regressor.fit(X_train,y_train)\n",
    "                                   \n",
    "# Predict the Response corresponding to Predictors\n",
    "y_train_pred = regressor.predict (X_train)\n",
    "y_test_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb913e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the small tree\n",
    "tree_small = regressor.estimators_[5]\n",
    "\n",
    "f = plt.figure(figsize=(12,12))\n",
    "tree_small = export_graphviz(tree_small, \n",
    "                             feature_names = X_list, \n",
    "                             rounded = True,  \n",
    "                             filled = True)\n",
    "\n",
    "import graphviz \n",
    "graphviz.Source(tree_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,4))\n",
    "prediction, = plt.plot(y_test_pred, label = \"prediction\")\n",
    "original, = plt.plot(y_test, label = 'original')\n",
    "first_legend= plt.legend(handles=[prediction])\n",
    "ax = plt.gca().add_artist(first_legend)\n",
    "plt.xlabel('id')\n",
    "plt.ylabel('happiness score')\n",
    "plt.legend(handles=[original],loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Regression Accuracy (R^2) \\t:\", regressor.score(X_train, y_train))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error (y_train, y_train_pred))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mean_squared_error (y_train, y_train_pred)))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Regression Accuracy (R^2) \\t:\", regressor.score(X_test, y_test))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error (y_test, y_test_pred))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mean_squared_error (y_test, y_test_pred)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce158cdb",
   "metadata": {},
   "source": [
    "> From the two model above we can see that the higher the max_depth of the tree the higher the accuracy and the lower the RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f8287",
   "metadata": {},
   "source": [
    "#### Variable Importances\n",
    ">Feature importances are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc644e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(regressor.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d3ba0",
   "metadata": {},
   "source": [
    "#### Variable Importances Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for plotting and use magic command for Jupyter Notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, X_list, rotation='vertical')\n",
    "\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52b373",
   "metadata": {},
   "source": [
    ">- There are only 14 variables that are used to split the tree. Among those, GDP per capita shows the most important variables compare to the others. \n",
    ">- We will choose only the top 7 independent variables with importance >=0.02 in our new random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d2cff",
   "metadata": {},
   "source": [
    "### B. Model with Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff42e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Region', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc6280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and lables \n",
    "y = df['happiness.score']\n",
    "X = df[['GDP per capita (current US$)', \n",
    "        'Life expectancy at birth (males, years)',\n",
    "        'Life expectancy at birth (females, years)',\n",
    "        'Education: Secondary gross enrol. ratio (female per 100 pop.)',\n",
    "        'Population age distribution (0-14 years, %)',\n",
    "        'Infant mortality rate (per 1000 live births', \n",
    "        'Urban population (% of total population)']]\n",
    "              \n",
    "X_list = list(X.columns)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "#Split the dataset into training and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "#Fitting the data\n",
    "regressor = RandomForestRegressor (n_estimators = 100, random_state = 0, max_depth = 20)\n",
    "regressor.fit (X_train,y_train)\n",
    "\n",
    "#Predicting the test set results \n",
    "y_train_pred = regressor.predict(X_train)\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#Error checking\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Regression Accuracy (R^2) \\t:\", regressor.score(X_train, y_train))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error (y_train, y_train_pred))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mean_squared_error (y_train, y_train_pred)))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Regression Accuracy (R^2) \\t:\", regressor.score(X_test, y_test))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error (y_test, y_test_pred))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mean_squared_error (y_test, y_test_pred)))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413dd23c",
   "metadata": {},
   "source": [
    ">The RMSE has reduced from 0.478 to 0.468 and the the accuracy (R^2) also improve from 80.1% to 80.8%. This shows that removing insignificant variables can improve the splitting of decision tree better even though not that large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9390c",
   "metadata": {},
   "source": [
    "### C. Model Based on Variables with Highest Correlation Values\n",
    ">Our initial approach is to use top 6 independent variables with the highest correlation values with happiness score. Hence, we will try to use random forest to predict whether this idea can improve the model accuracy or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95586c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and lables \n",
    "y = df['happiness.score']\n",
    "X = df[['Employment: Agriculture (% of employed)',\n",
    "       'Life expectancy at birth (females, years)',\n",
    "       'Employment: Services (% of employed)',\n",
    "       'Population age distribution (60+ years, %)',\n",
    "       'Education: Primary gross enrol. ratio (female per 100 pop.)']]\n",
    "\n",
    "                               \n",
    "X_list = list(X.columns)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "#Split the dataset into training and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "#Fitting the data\n",
    "regressor = RandomForestRegressor (n_estimators = 100, random_state = 0, max_depth =20)\n",
    "regressor.fit (X_train,y_train)\n",
    "\n",
    "\n",
    "#Predicting the test set results \n",
    "y_train_pred = regressor.predict(X_train)\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "#Error checking\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Regression Accuracy (R^2) \\t:\", regressor.score(X_train, y_train))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error (y_train, y_train_pred))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mean_squared_error (y_train, y_train_pred)))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Regression Accuracy (R^2) \\t:\", regressor.score(X_test, y_test))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error (y_test, y_test_pred))\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mean_squared_error (y_test, y_test_pred)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a6522",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy= pd.read_excel ('Error checking.xlsx', sheet_name = 'Sheet4')\n",
    "Accuracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = pd.read_excel ('Error checking.xlsx', sheet_name = 'Sheet3')\n",
    "error.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd651df",
   "metadata": {},
   "source": [
    ">There was no significant increase in the RMSE scores and accuracy scores. Instead it got worse. Hence, we can conclude that selecting independent variables based on the highest correlation values is not the best way to be used in random forest model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb7542",
   "metadata": {},
   "source": [
    "\n",
    "><b>Pros of random forest regressor: </b>\n",
    ">- Lower risk of overfitting due to randomness of data and feature selection\n",
    ">- Improve the accuracy\n",
    ">- Robust to outliers.\n",
    "\n",
    "><b>Cons of random forest regressor: </b>\n",
    ">- Slow training since it requires lots computational power amd resources since it builds greater amount of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0973e",
   "metadata": {},
   "source": [
    "## 3. K Means Clustering (Centroid Based)\n",
    "\n",
    "K-means is a centroid-based algorithm, or a distance-based algorithm. It calculate the distances to assign a point to a cluster. In K-Means, each cluster is associated with a centroid. Clustering gives the user the ability to understand how happy a country is at a simple glance of their group category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2717411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related libraries\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba96570a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19440/1173406926.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Drop non-float data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dataset.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"country\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Region\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset.csv'"
     ]
    }
   ],
   "source": [
    "#Drop non-float data\n",
    "data = pd.read_csv('Dataset.csv')\n",
    "data.drop(\"country\", axis=1, inplace=True)\n",
    "data.drop(\"Region\", axis=1, inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f29be6",
   "metadata": {},
   "source": [
    "### Scaling\n",
    ">Scaling is necessary as K means is a method that groups samples according to their distance from the chosen centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale all variables in dataset\n",
    "scaler = MinMaxScaler()\n",
    "for column in data:\n",
    "    scaler.fit(data[[column]])\n",
    "    data[column] = scaler.transform(data[[column]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55e5bf",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c801c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find variables most related to happiness\n",
    "f = plt.figure(figsize=(30, 30))\n",
    "sb.heatmap(data.corr(), vmin = -1, vmax = 1, linewidths = 1,\n",
    "           annot = True, fmt = \".2f\", annot_kws = {\"size\": 18}, cmap = \"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74fda1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picked 2 variable with highest correleation to happiness.score for clustering, dropping the labelled data happiness,\n",
    "# to simulate unlabeled data and perform unsupervised learning\n",
    "ml = pd.DataFrame(data[['Individuals using the Internet (per 100 inhabitants)', 'Education: Secondary gross enrol. ratio (female per 100 pop.)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.scatter(data['Individuals using the Internet (per 100 inhabitants)'], data['Education: Secondary gross enrol. ratio (female per 100 pop.)'] )\n",
    "plt.xlabel('Individuals using the Internet (per 100 inhabitants)')\n",
    "plt.ylabel('Secondary Education ratio (female per 100 pop.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a5c86",
   "metadata": {},
   "source": [
    "### Finding K\n",
    "> Finding K manuallty is required to perform K means clustering, this essentially decides how many clusters should the algorithm split the data into.\n",
    "\n",
    "> There are primarly different elbow methods, distortion, and inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "inertias = []\n",
    "mapping1 = {}\n",
    "mapping2 = {}\n",
    "K = range(1, 10)\n",
    " \n",
    "for k in K:\n",
    "    # Building and fitting the model\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(ml)\n",
    "    kmeanModel.fit(ml)\n",
    " \n",
    "    distortions.append(sum(np.min(cdist(ml, kmeanModel.cluster_centers_,\n",
    "                                        'euclidean'), axis=1)) / ml.shape[0])\n",
    "    inertias.append(kmeanModel.inertia_)\n",
    " \n",
    "    mapping1[k] = sum(np.min(cdist(ml, kmeanModel.cluster_centers_,\n",
    "                                   'euclidean'), axis=1)) / ml.shape[0]\n",
    "    mapping2[k] = kmeanModel.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in mapping1.items():\n",
    "    print(f'{key} : {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2dddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow method using distortion\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method using Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b7423",
   "metadata": {},
   "source": [
    "> Distortion is calculated as the average of the squared distances from the cluster centers of the respective clusters. Typically, the Euclidean distance metric is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457effc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in mapping2.items():\n",
    "    print(f'{key} : {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe22ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow method using inertia\n",
    "plt.plot(K, inertias, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The Elbow Method using Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ed92c",
   "metadata": {},
   "source": [
    "> Inertia is the sum of squared distances of samples to their closest cluster center."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9fb61",
   "metadata": {},
   "source": [
    ">One can see that in both the inertia distortion graph and the distortion graph, the point where the graph starts to turn linear would be a K = 5.\n",
    "\n",
    ">Therefore, using the elbow method our K value would be 5 this time round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7be9c",
   "metadata": {},
   "source": [
    "### Happiness Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits up happiness score based on k groups to test for cluster later on\n",
    "og = list()\n",
    "for score in data['happiness.score']:\n",
    "    if score <= 0.20:\n",
    "        og.append(0)\n",
    "    elif score <= 0.4:\n",
    "        og.append(1)\n",
    "    elif score <= 0.6:\n",
    "        og.append(2)\n",
    "    elif score <= 0.8:\n",
    "        og.append(3)\n",
    "    else:\n",
    "        og.append(4)\n",
    "data['cluster'] = pd.Series(og)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c77fba0",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e340368",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = 5)\n",
    "predict = km.fit_predict(ml)\n",
    "ml['group'] = predict\n",
    "center = km.cluster_centers_\n",
    "print(center)\n",
    "# Initliazed centroids that can be used in the next block to prevent randomness,\n",
    "# as cluster group is always random upon running algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole block used to rearrange clusters so that it will always be group 0 being lowest tier\n",
    "iter = [0,1,2,3,4]\n",
    "newgroup = list()\n",
    "newcluster = list()\n",
    "for i in iter:\n",
    "    if center[i][0] <= 0.20:\n",
    "        newgroup.append(0)\n",
    "    elif center[i][0] <= 0.4:\n",
    "        newgroup.append(1)\n",
    "    elif center[i][0] <= 0.6:\n",
    "        newgroup.append(2)\n",
    "    elif center[i][0] <= 0.8:\n",
    "        newgroup.append(3)\n",
    "    else:\n",
    "        newgroup.append(4)\n",
    "print(newgroup)\n",
    "for cluster in ml['group']:\n",
    "    newcluster.append(newgroup[cluster])\n",
    "ml['new'] = pd.Series(newcluster)\n",
    "ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data is sorted according to group\n",
    "ml1 =  ml[ml.new == 4]\n",
    "ml2 =  ml[ml.new == 3]\n",
    "ml3 =  ml[ml.new == 2]\n",
    "ml4 =  ml[ml.new == 1]\n",
    "ml5 =  ml[ml.new == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloring and printing centroid for visulaizing\n",
    "plt.scatter(ml1['Individuals using the Internet (per 100 inhabitants)'], ml1['Education: Secondary gross enrol. ratio (female per 100 pop.)'], color ='green')\n",
    "plt.scatter(ml2['Individuals using the Internet (per 100 inhabitants)'], ml2['Education: Secondary gross enrol. ratio (female per 100 pop.)'], color ='yellow')\n",
    "plt.scatter(ml3['Individuals using the Internet (per 100 inhabitants)'], ml3['Education: Secondary gross enrol. ratio (female per 100 pop.)'], color ='orange')\n",
    "plt.scatter(ml4['Individuals using the Internet (per 100 inhabitants)'], ml4['Education: Secondary gross enrol. ratio (female per 100 pop.)'], color ='red')\n",
    "plt.scatter(ml5['Individuals using the Internet (per 100 inhabitants)'], ml5['Education: Secondary gross enrol. ratio (female per 100 pop.)'], color ='blue')\n",
    "plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1], color = 'purple', marker='*', label= 'centroid')\n",
    "plt.xlabel('Internet percentage')\n",
    "plt.ylabel('Secondary Education ratio (female per 100 pop.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fd115",
   "metadata": {},
   "source": [
    "### Error checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml['cluster'] = pd.Series(og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fbb4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2fe698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing previously dropped happiness cluster with predicted cluster results\n",
    "wrong = 0\n",
    "for index, row in ml.iterrows():\n",
    "    if row['new'] != row['cluster']:\n",
    "        wrong += 1\n",
    "print(\"Total rightly predicted : \" + str(len(ml)-wrong) + \" out of \" + str(len(ml)))\n",
    "print(\"Total accuracy is : \" + str((len(ml)-wrong)/len(ml)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672b31e",
   "metadata": {},
   "source": [
    "> <b>Pros of KMeans Clustering :</b>\n",
    ">- Relatively simple to implement, this is especially important for new analyst such as ourselves to be able to understand how it works to be able to implement the machine learning to our liking\n",
    ">- Ease of interpretation and visualization\n",
    "\n",
    "><b>Cons of KMeans Clustering :</b>\n",
    ">-  No optimal set of clusters and would require one manually choose K as seen from the distortion and inertia method\n",
    ">- Sensitive to outlier data as their entroids can be dragged by outliers instead of being ignored, having the need to remove outliers first before clustering\n",
    ">-  Inconsistency can happen when K-Means algorithm picks random centroid initialization to develop the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730ffbf",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "> K means clustering is the obvious inferior machine learning situation here as it specializes in unsupervied learning with unlabeled data. However in this case, the happiness score is provided and hence not necessary to be limited to unsupervised learning.This is especially the case for the K means which has a astounding low accuracy this time round\n",
    "\n",
    "> Moving on to the other 2 supervised learning models, if we compare the random forest with the ElasticNet regression model, the accuracy of the random forest (with most important variables) is much higher (59.3% to 80.4%) and has a significantly lower RMSE value (0.543 to 0.469). \n",
    "\n",
    ">Hence, random forest regressor is a better model than Elasticnet Regression when it comes to predicting this happiness score dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
